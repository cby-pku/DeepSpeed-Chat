{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from petrel_client.client import Client\n",
    "import os\n",
    "import io\n",
    "import torch\n",
    "client = Client()\n",
    "\n",
    "ckpts = [f'tp_{idx}.pt' for idx in range(8)]\n",
    "states = []\n",
    "root_file = \"anonymous_ssd:s3://model_weights/0331/evaluation/exported_llama/1006/12499/\"\n",
    "for ckpt in ckpts:\n",
    "    config_file = os.path.join(root_file, ckpt)\n",
    "    with io.BytesIO(client.get(config_file)) as f:      \n",
    "        state = torch.load(f, map_location='cpu')\n",
    "    states.append(state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dim': 10240,\n",
       " 'multiple_of': 256,\n",
       " 'n_heads': 80,\n",
       " 'n_layers': 82,\n",
       " 'norm_eps': 1e-05,\n",
       " 'vocab_size': -1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaConfig\n",
    "import json\n",
    "with io.BytesIO(client.get(root_file+'params.json')) as f:\n",
    "    params = json.loads(f.getvalue())\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 10240,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 27392,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 80,\n",
       "  \"num_hidden_layers\": 82,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.28.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 65632\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_intermediate_size(params):\n",
    "    hidden_dim = params['dim'] * 4\n",
    "    hidden_dim = int(2 * hidden_dim / 3)\n",
    "    hidden_dim = params['multiple_of'] * ((hidden_dim + params['multiple_of'] - 1) // params['multiple_of'])\n",
    "    return hidden_dim\n",
    "llama_config = LlamaConfig(vocab_size=65632,\n",
    "                    hidden_size=params['dim'],\n",
    "                    intermediate_size=get_intermediate_size(params),\n",
    "                    num_hidden_layers=params['n_layers'],\n",
    "                    num_attention_heads=params['n_heads']\n",
    "                 )\n",
    "llama_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "llama_model = LlamaForCausalLM(llama_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转换（同步）权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def fuse_weight(states, key, dim):\n",
    "    return torch.cat([state[key] for state in states], dim=dim)\n",
    "\n",
    "def load_attention_weight(layer, idx, states):\n",
    "    new_q_proj = fuse_weight(states, f'layers.{idx}.attention.wq.weight', dim=0)\n",
    "    assert new_q_proj.shape == layer.self_attn.q_proj.weight.shape\n",
    "    layer.self_attn.q_proj.weight = torch.nn.Parameter(new_q_proj)\n",
    "    \n",
    "    new_k_proj = fuse_weight(states, f'layers.{idx}.attention.wk.weight', dim=0)\n",
    "    assert new_k_proj.shape == layer.self_attn.k_proj.weight.shape\n",
    "    layer.self_attn.k_proj.weight = torch.nn.Parameter(new_k_proj)\n",
    "    \n",
    "    new_v_proj = fuse_weight(states, f'layers.{idx}.attention.wv.weight', dim=0)\n",
    "    assert new_v_proj.shape == layer.self_attn.v_proj.weight.shape\n",
    "    layer.self_attn.v_proj.weight = torch.nn.Parameter(new_v_proj)\n",
    "    \n",
    "    new_o_proj = fuse_weight(states, f'layers.{idx}.attention.wo.weight', dim=1)\n",
    "    assert new_o_proj.shape == layer.self_attn.o_proj.weight.shape\n",
    "    layer.self_attn.o_proj.weight = torch.nn.Parameter(new_o_proj)\n",
    "\n",
    "def load_feedforward_weight(layer, idx, states):\n",
    "    new_w1_weight = fuse_weight(states, f'layers.{idx}.feed_forward.w1.weight', dim=0)\n",
    "    assert new_w1_weight.shape == layer.mlp.gate_proj.weight.shape\n",
    "    layer.mlp.gate_proj.weight = torch.nn.Parameter(new_w1_weight)\n",
    "    \n",
    "    new_w2_weight = fuse_weight(states, f'layers.{idx}.feed_forward.w2.weight', dim=1)\n",
    "    assert new_w2_weight.shape == layer.mlp.down_proj.weight.shape\n",
    "    layer.mlp.down_proj.weight = torch.nn.Parameter(new_w2_weight)\n",
    "    \n",
    "    new_w3_weight = fuse_weight(states, f'layers.{idx}.feed_forward.w3.weight', dim=0)\n",
    "    assert new_w3_weight.shape == layer.mlp.up_proj.weight.shape\n",
    "    layer.mlp.up_proj.weight = torch.nn.Parameter(new_w3_weight)\n",
    "    \n",
    "def load_norm_weight(layer, idx, states):\n",
    "    layer.input_layernorm.weight = torch.nn.Parameter(states[0][f'layers.{idx}.attention_norm.weight'])\n",
    "    layer.post_attention_layernorm.weight = torch.nn.Parameter(states[0][f'layers.{idx}.ffn_norm.weight'])\n",
    "    \n",
    "def load_embed_tokens(model, states):\n",
    "    new_em_weight = torch.cat([state['tok_embeddings.weight'] for state in states], dim=1)\n",
    "    assert new_em_weight.shape == model.embed_tokens.weight.shape\n",
    "    model.embed_tokens.weight = torch.nn.Parameter(new_em_weight)\n",
    "    \n",
    "def load_head_weight(llama_model, states):\n",
    "    llama_model.lm_head.weight = torch.nn.Parameter(fuse_weight(states, f'output.weight', dim=0))\n",
    "    \n",
    "def sync_weight(llama_model, states):\n",
    "    load_embed_tokens(llama_model.model, states)\n",
    "    llama_model.model.norm.weight = torch.nn.Parameter(states[0][f'norm.weight'])\n",
    "    load_head_weight(llama_model, states)\n",
    "    for idx, layer in enumerate(llama_model.model.layers):\n",
    "        load_attention_weight(layer, idx, states)\n",
    "        load_feedforward_weight(layer, idx, states)\n",
    "        load_norm_weight(layer, idx, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "sync_weight(llama_model=llama_model, states=states)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "save_path = '/mnt/petrelfs/wangzerui/DeepSpeed/DeepSpeedExamples/applications/DeepSpeed-Chat/llama_model/7132k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "llama_model.save_pretrained(save_path, max_shard_size='2GB')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/petrelfs/wangzerui/DeepSpeed/DeepSpeedExamples/.conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1713: FutureWarning: Calling LlamaTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/mnt/petrelfs/wangzerui/DeepSpeed/DeepSpeedExamples/applications/DeepSpeed-Chat/llama_model/7132k/tokenizer_config.json',\n",
       " '/mnt/petrelfs/wangzerui/DeepSpeed/DeepSpeedExamples/applications/DeepSpeed-Chat/llama_model/7132k/special_tokens_map.json',\n",
       " '/mnt/petrelfs/wangzerui/DeepSpeed/DeepSpeedExamples/applications/DeepSpeed-Chat/llama_model/7132k/tokenizer.model',\n",
       " '/mnt/petrelfs/wangzerui/DeepSpeed/DeepSpeedExamples/applications/DeepSpeed-Chat/llama_model/7132k/added_tokens.json')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained('/mnt/petrelfs/wangzerui/DeepSpeed/DeepSpeedExamples/llamav4.model')\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained('/mnt/petrelfs/wangzerui/DeepSpeed/DeepSpeedExamples/applications/DeepSpeed-Chat/llama_model/7132k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829d57811fcb4709902ce62b4608f2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m LlamaForCausalLM\n\u001b[0;32m----> 2\u001b[0m llama_model \u001b[39m=\u001b[39m LlamaForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39m/mnt/petrelfs/wangzerui/DeepSpeed/DeepSpeedExamples/applications/DeepSpeed-Chat/llama_model/7132k\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/DeepSpeed/DeepSpeedExamples/.conda/lib/python3.8/site-packages/transformers/modeling_utils.py:2795\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2786\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   2788\u001b[0m     (\n\u001b[1;32m   2789\u001b[0m         model,\n\u001b[1;32m   2790\u001b[0m         missing_keys,\n\u001b[1;32m   2791\u001b[0m         unexpected_keys,\n\u001b[1;32m   2792\u001b[0m         mismatched_keys,\n\u001b[1;32m   2793\u001b[0m         offload_index,\n\u001b[1;32m   2794\u001b[0m         error_msgs,\n\u001b[0;32m-> 2795\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   2796\u001b[0m         model,\n\u001b[1;32m   2797\u001b[0m         state_dict,\n\u001b[1;32m   2798\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   2799\u001b[0m         resolved_archive_file,\n\u001b[1;32m   2800\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   2801\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   2802\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   2803\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   2804\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   2805\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   2806\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   2807\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   2808\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   2809\u001b[0m         load_in_8bit\u001b[39m=\u001b[39;49mload_in_8bit,\n\u001b[1;32m   2810\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   2811\u001b[0m     )\n\u001b[1;32m   2813\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n\u001b[1;32m   2815\u001b[0m \u001b[39m# make sure token embedding weights are still tied if needed\u001b[39;00m\n",
      "File \u001b[0;32m~/DeepSpeed/DeepSpeedExamples/.conda/lib/python3.8/site-packages/transformers/modeling_utils.py:3141\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, load_in_8bit, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3139\u001b[0m     error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_error_msgs\n\u001b[1;32m   3140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3141\u001b[0m     error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m _load_state_dict_into_model(model_to_load, state_dict, start_prefix)\n\u001b[1;32m   3143\u001b[0m \u001b[39m# force memory release\u001b[39;00m\n\u001b[1;32m   3144\u001b[0m \u001b[39mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m~/DeepSpeed/DeepSpeedExamples/.conda/lib/python3.8/site-packages/transformers/modeling_utils.py:529\u001b[0m, in \u001b[0;36m_load_state_dict_into_model\u001b[0;34m(model_to_load, state_dict, start_prefix)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    527\u001b[0m             load(child, state_dict, prefix \u001b[39m+\u001b[39m name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 529\u001b[0m load(model_to_load, state_dict, prefix\u001b[39m=\u001b[39;49mstart_prefix)\n\u001b[1;32m    530\u001b[0m \u001b[39m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[39m# it's safe to delete it.\u001b[39;00m\n\u001b[1;32m    532\u001b[0m \u001b[39mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m~/DeepSpeed/DeepSpeedExamples/.conda/lib/python3.8/site-packages/transformers/modeling_utils.py:527\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    526\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m         load(child, state_dict, prefix \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/DeepSpeed/DeepSpeedExamples/.conda/lib/python3.8/site-packages/transformers/modeling_utils.py:527\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    526\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m         load(child, state_dict, prefix \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "    \u001b[0;31m[... skipping similar frames: _load_state_dict_into_model.<locals>.load at line 527 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/DeepSpeed/DeepSpeedExamples/.conda/lib/python3.8/site-packages/transformers/modeling_utils.py:527\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    526\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m         load(child, state_dict, prefix \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/DeepSpeed/DeepSpeedExamples/.conda/lib/python3.8/site-packages/transformers/modeling_utils.py:523\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    521\u001b[0m                     module\u001b[39m.\u001b[39m_load_from_state_dict(\u001b[39m*\u001b[39margs)\n\u001b[1;32m    522\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 523\u001b[0m         module\u001b[39m.\u001b[39;49m_load_from_state_dict(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    525\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    526\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/DeepSpeed/DeepSpeedExamples/.conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1942\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   1940\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1941\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> 1942\u001b[0m         param\u001b[39m.\u001b[39;49mcopy_(input_param)\n\u001b[1;32m   1943\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m   1944\u001b[0m     error_msgs\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39mWhile copying the parameter named \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1945\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39mwhose dimensions in the model are \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1946\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39mwhose dimensions in the checkpoint are \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1947\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39man exception occurred : \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1948\u001b[0m                       \u001b[39m.\u001b[39mformat(key, param\u001b[39m.\u001b[39msize(), input_param\u001b[39m.\u001b[39msize(), ex\u001b[39m.\u001b[39margs))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "llama_model = LlamaForCausalLM.from_pretrained('/mnt/petrelfs/wangzerui/DeepSpeed/DeepSpeedExamples/applications/DeepSpeed-Chat/llama_model/7132k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'周杰伦是华语乐坛最有影响力的歌手之一,他的歌曲传唱红遍大江南北国内外'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"周杰伦是华语乐坛最\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "generate_ids = llama_model.generate(inputs.input_ids, max_length=30)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上传权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00001-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00002-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00003-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00004-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00005-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00006-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00007-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00008-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00009-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00010-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00011-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00012-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00013-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00014-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00015-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00016-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00017-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00018-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00019-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00020-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00021-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00022-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00023-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00024-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00025-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00026-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00027-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00028-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00029-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00030-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00031-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00032-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00033-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00034-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00035-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00036-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00037-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00038-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00039-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00040-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00041-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00042-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00043-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00044-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00045-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00046-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00047-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00048-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00049-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00050-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00051-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00052-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00053-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00054-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00055-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00056-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00057-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00058-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00059-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00060-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00061-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00062-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00063-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00064-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00065-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00066-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00067-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00068-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00069-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00070-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00071-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00072-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00073-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00074-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00075-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00076-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00077-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00078-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00079-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00080-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00081-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00082-of-00083.bin\n",
      "Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/pytorch_model-00083-of-00083.bin\n"
     ]
    }
   ],
   "source": [
    "from petrel_client.client import Client\n",
    "import torch\n",
    "client = Client()\n",
    "def save_func(model, path):\n",
    "    ROOT_PATH = 'Sproject_ssd_02:s3://debug_ssd_02/wangzerui/'\n",
    "    full_path = ROOT_PATH + path\n",
    "    print(full_path)\n",
    "    buffer = io.BytesIO()\n",
    "    torch.save(model, buffer)\n",
    "    client.put(full_path, buffer.getvalue())\n",
    "    \n",
    "llama_model.save_pretrained('7132k', save_function=save_func, max_shard_size='3GB')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, LlamaForCausalLM\n",
    "local_config_path = '/mnt/petrelfs/wangzerui/DeepSpeed/DeepSpeedExamples/applications/DeepSpeed-Chat/llama_model/100B'\n",
    "model_config = AutoConfig.from_pretrained(local_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model = LlamaForCausalLM(model_config)\n",
    "model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from functools import partial\n",
    "from petrel_client.client import Client\n",
    "import os\n",
    "import torch\n",
    "import io\n",
    "\n",
    "def load_sharded_checkpoint(model, folder, s3_root, client):\n",
    "    \"\"\"\n",
    "    This is the same as\n",
    "    [`torch.nn.Module.load_state_dict`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=load_state_dict#torch.nn.Module.load_state_dict)\n",
    "    but for a sharded checkpoint.\n",
    "\n",
    "    This load is performed efficiently: each checkpoint shard is loaded one by one in RAM and deleted after being\n",
    "    loaded in the model.\n",
    "\n",
    "    Args:\n",
    "        model (`torch.nn.Module`): The model in which to load the checkpoint.\n",
    "        folder (`str` or `os.PathLike`): A path to a folder containing the sharded checkpoint.\n",
    "        strict (`bool`, *optional`, defaults to `True`):\n",
    "            Whether to strictly enforce that the keys in the model state dict match the keys in the sharded checkpoint.\n",
    "        prefer_safe (`bool`, *optional*, defaults to `False`)\n",
    "            If both safetensors and PyTorch save files are present in checkpoint and `prefer_safe` is True, the\n",
    "            safetensors files will be loaded. Otherwise, PyTorch files are always loaded when possible.\n",
    "\n",
    "    Returns:\n",
    "        `NamedTuple`: A named tuple with `missing_keys` and `unexpected_keys` fields\n",
    "            - `missing_keys` is a list of str containing the missing keys\n",
    "            - `unexpected_keys` is a list of str containing the unexpected keys\n",
    "    \"\"\"\n",
    "    # Load the index\n",
    "    index_file = os.path.join(folder, \"pytorch_model.bin.index.json\")\n",
    "\n",
    "    index_present = os.path.isfile(index_file)\n",
    "    assert index_present\n",
    "    assert s3_root.endswith('/')\n",
    "    load_index = index_file\n",
    "\n",
    "    with open(load_index, \"r\", encoding=\"utf-8\") as f:\n",
    "        index = json.load(f)\n",
    "\n",
    "    shard_files = list(set(index[\"weight_map\"].values()))\n",
    "\n",
    "    # If strict=True, error before loading any of the state dicts.\n",
    "    loaded_keys = index[\"weight_map\"].keys()\n",
    "    model_keys = model.state_dict().keys()\n",
    "    missing_keys = [key for key in model_keys if key not in loaded_keys]\n",
    "    unexpected_keys = [key for key in loaded_keys if key not in model_keys]\n",
    "\n",
    "    loader = partial(torch.load, map_location=\"cpu\")\n",
    "\n",
    "    for shard_file in shard_files:\n",
    "        s3_shard_file_path = s3_root + shard_file\n",
    "        print(s3_shard_file_path)\n",
    "        with io.BytesIO(client.get(s3_shard_file_path)) as f:\n",
    "            state_dict = loader(f)\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "            # Make sure memory is freed before we load the next state dict.\n",
    "            del state_dict\n",
    "client = Client()\n",
    "s3_root = 'Sproject_ssd_02:s3://debug_ssd_02/wangzerui/7132k/'\n",
    "load_sharded_checkpoint(model=model, folder=local_config_path, client=client, s3_root=s3_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'周杰伦是华语乐坛最成功的歌手之一，也是华语乐坛的领军人物，他的歌曲，他的歌曲，他的歌曲，他的歌曲，他的歌曲'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"周杰伦是华语乐坛最\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
